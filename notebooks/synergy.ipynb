{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "The goal of this notebook is to compute synergy metrics between the stores/retailers/categories based on the cross visit data. We will use graph-based methods to analyze the relationships and interactions between different entities in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants.constants as cst\n",
    "import constants.paths as pth\n",
    "\n",
    "# Dim Tables\n",
    "dim_blocks = pd.read_csv(pth.DIM_BLOCKS, **cst.CSV_PARAMS)\n",
    "dim_malls = pd.read_csv(pth.DIM_MALLS, **cst.CSV_PARAMS)\n",
    "\n",
    "# Fact Tables\n",
    "fact_stores = pd.read_csv(pth.FACT_STORES, **cst.CSV_PARAMS)\n",
    "fact_malls = pd.read_csv(pth.FACT_MALLS, **cst.CSV_PARAMS)\n",
    "fact_sri_scores = pd.read_csv(pth.FACT_SRI_SCORES, **cst.CSV_PARAMS)\n",
    "\n",
    "# Store financials table\n",
    "store_financials = pd.read_csv(pth.STORE_FINANCIALS, **cst.CSV_PARAMS)\n",
    "\n",
    "# Cross visit table\n",
    "cross_visit = pd.read_csv(pth.CROSS_VISITS, **cst.CSV_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data enriching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_blocks[dim_blocks[\"store_code\"].duplicated()].sort_values(\"store_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We want to compute synergy metrics at the store level, retailer level and category level. For that, we need to enrich the cross visit data with retailer and category information. Additionally, to build graphs per mall, we add the mall id to the enriching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_1_enrich = (\n",
    "    dim_blocks[\n",
    "        [\n",
    "            \"store_code\",\n",
    "            \"mall_id\",\n",
    "            \"retailer_code\",\n",
    "            \"bl1_label\",\n",
    "            \"bl2_label\",\n",
    "            \"bl3_label\",\n",
    "        ]\n",
    "    ]\n",
    "    .drop_duplicates(\"store_code\")\n",
    "    .add_suffix(\"_1\")\n",
    ")\n",
    "\n",
    "store_2_enrich = (\n",
    "    dim_blocks[\n",
    "        [\n",
    "            \"store_code\",\n",
    "            \"mall_id\",\n",
    "            \"retailer_code\",\n",
    "            \"bl1_label\",\n",
    "            \"bl2_label\",\n",
    "            \"bl3_label\",\n",
    "        ]\n",
    "    ]\n",
    "    .drop_duplicates(\"store_code\")\n",
    "    .add_suffix(\"_2\")\n",
    ")\n",
    "\n",
    "cross_visit_enriched = pd.merge(\n",
    "    cross_visit,\n",
    "    store_1_enrich,\n",
    "    left_on=\"store_code_1\",\n",
    "    right_on=\"store_code_1\",\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\",\n",
    ")\n",
    "\n",
    "cross_visit_enriched = pd.merge(\n",
    "    cross_visit_enriched,\n",
    "    store_2_enrich,\n",
    "    left_on=\"store_code_2\",\n",
    "    right_on=\"store_code_2\",\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We check that there is no error in the `mall_id` (no mismatching `mall_id`). The only differences come from when one store has a mall_id and the other does not. Thus, we can combine the `mall_id` columns to get a full one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_visit_enriched[\n",
    "    (cross_visit_enriched[\"mall_id_1\"] != cross_visit_enriched[\"mall_id_2\"])\n",
    "    & (\n",
    "        cross_visit_enriched[\"mall_id_1\"].notna()\n",
    "        & cross_visit_enriched[\"mall_id_2\"].notna()\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_visit_enriched[\"mall_id\"] = cross_visit_enriched[\"mall_id_1\"].combine_first(\n",
    "    cross_visit_enriched[\"mall_id_2\"]\n",
    ")\n",
    "\n",
    "cross_visit_enriched = cross_visit_enriched.drop(columns=[\"mall_id_1\", \"mall_id_2\"])\n",
    "\n",
    "# Drop rows where mall_id is missing alltogether\n",
    "cross_visit_enriched = cross_visit_enriched.dropna(axis=0, subset=\"mall_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_visit_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We still need to normalize the edge weights to have comparable values. We can do:\n",
    "$$\n",
    "edge\\_weight_{ij} = \\frac{cross\\_total\\_cross\\_visits_{ij}}{\\sqrt{total\\_visits_i \\times {total\\_visits_j}}}\n",
    "$$\n",
    "\n",
    "At this point, the issue is that for some stores, there are more cross visits in `cross_visits` than total visits in `fact_stores`... Ask the question, but maybe use sum of cross visits as proxy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(data: pd.DataFrame, mall_id: int, granularity: str) -> nx.Graph:\n",
    "    \"\"\"Construct a graph for a specific mall and granularity level.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing cross visit data.\n",
    "    - mall_id: The mall ID to filter the data.\n",
    "    - granularity: The granularity level. Must be one of\n",
    "                   ('store', 'retailer', 'cat_high', 'cat_mid', 'cat_low').\n",
    "\n",
    "    Returns:\n",
    "    - A NetworkX graph object.\n",
    "    \"\"\"\n",
    "    if granularity == \"store\":\n",
    "        node_col_1 = \"store_code_1\"\n",
    "        node_col_2 = \"store_code_2\"\n",
    "    elif granularity == \"retailer\":\n",
    "        node_col_1 = \"retailer_code_1\"\n",
    "        node_col_2 = \"retailer_code_2\"\n",
    "    elif granularity == \"cat_high\":\n",
    "        node_col_1 = \"bl1_label_1\"\n",
    "        node_col_2 = \"bl1_label_2\"\n",
    "    elif granularity == \"cat_mid\":\n",
    "        node_col_1 = \"bl2_label_1\"\n",
    "        node_col_2 = \"bl2_label_2\"\n",
    "    elif granularity == \"cat_low\":\n",
    "        node_col_1 = \"bl3_label_1\"\n",
    "        node_col_2 = \"bl3_label_2\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Granularity must be one of 'store', 'retailer', 'cat_high', 'cat_mid' \"\n",
    "            \"or 'cat_low'.\"\n",
    "        )\n",
    "\n",
    "    mall_data = data[data[\"mall_id\"] == mall_id]\n",
    "\n",
    "    graph = nx.from_pandas_edgelist(\n",
    "        mall_data,\n",
    "        source=node_col_1,\n",
    "        target=node_col_2,\n",
    "        edge_attr=\"total_cross_visits\",\n",
    "        create_using=nx.Graph(),\n",
    "    )\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_metrics(graph: nx.Graph) -> pd.DataFrame:\n",
    "    \"\"\"Compute graph metrics for each node in the graph.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: A NetworkX graph object (typically from construct_graph).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with node metrics indexed by node ID.\n",
    "    \"\"\"\n",
    "    # Weighted degree (strength) - total cross-visit volume\n",
    "    weighted_degree = dict(graph.degree(weight=\"total_cross_visits\"))\n",
    "\n",
    "    # Unweighted degree - number of connections\n",
    "    degree = dict(graph.degree())\n",
    "\n",
    "    # Degree centrality (normalized)\n",
    "    degree_centrality = nx.degree_centrality(graph)\n",
    "\n",
    "    # Betweenness centrality - bridge nodes connecting clusters\n",
    "    # Use weight inversion: high cross-visits = short distance\n",
    "    betweenness = nx.betweenness_centrality(\n",
    "        graph, weight=\"total_cross_visits\", normalized=True\n",
    "    )\n",
    "\n",
    "    # PageRank - importance via incoming flow\n",
    "    pagerank = nx.pagerank(graph, weight=\"total_cross_visits\")\n",
    "\n",
    "    # Clustering coefficient - local clustering tightness\n",
    "    clustering = nx.clustering(graph, weight=\"total_cross_visits\")\n",
    "\n",
    "    # Eigenvector centrality - connected to well-connected nodes\n",
    "    try:\n",
    "        eigenvector = nx.eigenvector_centrality(\n",
    "            graph, weight=\"total_cross_visits\", max_iter=1000\n",
    "        )\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        eigenvector = {node: float(\"nan\") for node in graph.nodes()}\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            \"degree\": degree,\n",
    "            \"weighted_degree\": weighted_degree,\n",
    "            \"degree_centrality\": degree_centrality,\n",
    "            \"betweenness_centrality\": betweenness,\n",
    "            \"pagerank\": pagerank,\n",
    "            \"clustering_coefficient\": clustering,\n",
    "            \"eigenvector_centrality\": eigenvector,\n",
    "        }\n",
    "    )\n",
    "    metrics_df.index.name = \"node_id\"\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_metrics(graph: nx.Graph) -> dict:\n",
    "    \"\"\"Compute mall-level (graph-level) synergy metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: A NetworkX graph object (typically from construct_graph).\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with graph-level metrics.\n",
    "    \"\"\"\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Basic graph properties\n",
    "    metrics[\"n_nodes\"] = graph.number_of_nodes()\n",
    "    metrics[\"n_edges\"] = graph.number_of_edges()\n",
    "    metrics[\"density\"] = nx.density(graph)\n",
    "\n",
    "    # Degree statistics\n",
    "    degrees = [d for _, d in graph.degree()]\n",
    "    weighted_degrees = [d for _, d in graph.degree(weight=\"total_cross_visits\")]\n",
    "\n",
    "    metrics[\"avg_degree\"] = np.mean(degrees) if degrees else 0\n",
    "    metrics[\"avg_weighted_degree\"] = (\n",
    "        np.mean(weighted_degrees) if weighted_degrees else 0\n",
    "    )\n",
    "    metrics[\"std_weighted_degree\"] = np.std(weighted_degrees) if weighted_degrees else 0\n",
    "\n",
    "    # Gini coefficient of weighted degree (measures concentration)\n",
    "    if weighted_degrees and sum(weighted_degrees) > 0:\n",
    "        sorted_degrees = np.sort(weighted_degrees)\n",
    "        n = len(sorted_degrees)\n",
    "        cumsum = np.cumsum(sorted_degrees)\n",
    "        metrics[\"gini_weighted_degree\"] = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "    else:\n",
    "        metrics[\"gini_weighted_degree\"] = 0\n",
    "\n",
    "    # Top-k concentration (share of total weighted degree held by top 5 nodes)\n",
    "    if weighted_degrees:\n",
    "        total = sum(weighted_degrees)\n",
    "        top_k = sorted(weighted_degrees, reverse=True)[:5]\n",
    "        metrics[\"top5_degree_share\"] = sum(top_k) / total if total > 0 else 0\n",
    "    else:\n",
    "        metrics[\"top5_degree_share\"] = 0\n",
    "\n",
    "    # Clustering\n",
    "    metrics[\"avg_clustering\"] = nx.average_clustering(\n",
    "        graph, weight=\"total_cross_visits\"\n",
    "    )\n",
    "\n",
    "    # Transitivity (global clustering coefficient)\n",
    "    metrics[\"transitivity\"] = nx.transitivity(graph)\n",
    "\n",
    "    # Assortativity - do high-degree nodes connect to high-degree nodes?\n",
    "    try:\n",
    "        metrics[\"degree_assortativity\"] = nx.degree_assortativity_coefficient(graph)\n",
    "    except ValueError:\n",
    "        metrics[\"degree_assortativity\"] = float(\"nan\")\n",
    "\n",
    "    # Community detection and modularity\n",
    "    try:\n",
    "        communities = louvain_communities(graph, weight=\"total_cross_visits\", seed=42)\n",
    "        metrics[\"n_communities\"] = len(communities)\n",
    "        metrics[\"modularity\"] = nx.community.modularity(\n",
    "            graph, communities, weight=\"total_cross_visits\"\n",
    "        )\n",
    "    except Exception:\n",
    "        metrics[\"n_communities\"] = float(\"nan\")\n",
    "        metrics[\"modularity\"] = float(\"nan\")\n",
    "\n",
    "    # Connected components\n",
    "    metrics[\"n_connected_components\"] = nx.number_connected_components(graph)\n",
    "    metrics[\"is_connected\"] = nx.is_connected(graph)\n",
    "\n",
    "    # Average shortest path length (only if connected)\n",
    "    if metrics[\"is_connected\"] and metrics[\"n_nodes\"] > 1:\n",
    "        try:\n",
    "            metrics[\"avg_path_length\"] = nx.average_shortest_path_length(graph)\n",
    "        except Exception:\n",
    "            metrics[\"avg_path_length\"] = float(\"nan\")\n",
    "    else:\n",
    "        metrics[\"avg_path_length\"] = float(\"nan\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_graph = construct_graph(cross_visit_enriched, mall_id=22, granularity=\"store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Node-level metrics (per store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_node_metrics = compute_node_metrics(store_graph)\n",
    "store_node_metrics.sort_values(\"weighted_degree\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Graph-level metrics (per mall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_graph_metrics = compute_graph_metrics(store_graph)\n",
    "pd.Series(mall_graph_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "17-urw-data-challenge (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
